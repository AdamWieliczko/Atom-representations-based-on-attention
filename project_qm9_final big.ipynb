{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tdc.single_pred.adme import ADME\n",
    "from tdc import Evaluator\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self, y_column, smiles_col='Drug', **kwargs):\n",
    "        self.y_column = y_column\n",
    "        self.smiles_col = smiles_col\n",
    "        self.__dict__.update(kwargs)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise ValueError(\"input {0} not in allowable set{1}:\".format(\n",
    "            x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "class GraphFeaturizer(Featurizer):\n",
    "    def __call__(self, df, getRepresentation):\n",
    "        graphs = []\n",
    "        labels = []\n",
    "        for i, row in df.iterrows():\n",
    "            y = row[self.y_column]\n",
    "            smiles = row[self.smiles_col]\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            \n",
    "            edges = []\n",
    "            for bond in mol.GetBonds():\n",
    "                begin = bond.GetBeginAtomIdx()\n",
    "                end = bond.GetEndAtomIdx()\n",
    "                edges.append((begin, end))  # TODO: Add edges in both directions\n",
    "            edges = np.array(edges)\n",
    "            \n",
    "            nodes = []\n",
    "            for atom in mol.GetAtoms():\n",
    "                # print(atom.GetAtomicNum(), atom.GetNumImplicitHs(), atom.GetTotalNumHs(), atom.GetSymbol(), atom.GetNumExplicitHs(), atom.GetTotalValence())\n",
    "                results = getRepresentation(atom)\n",
    "                # print(results)\n",
    "                nodes.append(results)\n",
    "            nodes = np.array(nodes)\n",
    "            \n",
    "            graphs.append((nodes, edges.T))\n",
    "            labels.append(y)\n",
    "        labels = np.array(labels)\n",
    "        return [Data(\n",
    "            x=torch.FloatTensor(x), \n",
    "            edge_index=torch.LongTensor(edge_index), \n",
    "            y=torch.FloatTensor([y])\n",
    "        ) for ((x, edge_index), y) in zip(graphs, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultRepresentation(atom):\n",
    "    return one_of_k_encoding_unk(atom.GetAtomicNum(), range(11)) + one_of_k_encoding(\n",
    "                    atom.GetDegree(), range(11)\n",
    "                ) + one_of_k_encoding_unk(\n",
    "                    atom.GetImplicitValence(), range(11)\n",
    "                ) + [atom.GetIsAromatic()] + one_of_k_encoding_unk(\n",
    "                    atom.GetTotalNumHs(), range(11)\n",
    "                ) + [atom.GetNumImplicitHs(), atom.GetFormalCharge(), atom.GetNumRadicalElectrons(), atom.IsInRing()] # TODO: Add atom features as a list, you can use one_of_k_encodings defined above\n",
    "\n",
    "def representation1(atom):\n",
    "    return one_of_k_encoding_unk(atom.GetAtomicNum(), range(12)) + one_of_k_encoding_unk(\n",
    "                    atom.GetDegree(), range(6)) + one_of_k_encoding_unk(\n",
    "                    atom.GetTotalNumHs(), range(5)\n",
    "                ) + [atom.GetFormalCharge(), atom.IsInRing(), atom.GetIsAromatic()]\n",
    "\n",
    "def representation10(atom):\n",
    "    return one_of_k_encoding_unk(atom.GetAtomicNum(), range(12)) + one_of_k_encoding_unk(\n",
    "                    atom.GetDegree(), range(6)) + one_of_k_encoding_unk(\n",
    "                    atom.GetTotalNumHs(), range(5)\n",
    "                ) + [atom.IsInRing(), atom.GetIsAromatic()]\n",
    "\n",
    "def representationAll(atom):\n",
    "    return one_of_k_encoding_unk(atom.GetAtomicNum(), range(12)) + one_of_k_encoding_unk(\n",
    "                    atom.GetDegree(), range(6)) + one_of_k_encoding_unk(\n",
    "                    atom.GetTotalNumHs(), range(5)) + one_of_k_encoding_unk(\n",
    "                    atom.GetImplicitValence(), range(6))  + one_of_k_encoding_unk(\n",
    "                    atom.GetHybridization(),\n",
    "                    [\n",
    "                        Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
    "                        Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D,\n",
    "                        Chem.rdchem.HybridizationType.SP3D2\n",
    "                    ]\n",
    "                ) + [atom.GetFormalCharge(), atom.IsInRing(), atom.GetIsAromatic()\n",
    "                     ] + [atom.GetNumRadicalElectrons()]\n",
    "\n",
    "def printProperties(atom):\n",
    "    print(\"=========\")\n",
    "    print(\"GetDegree\", atom.GetDegree())\n",
    "    print(\"GetImplicitValence\", atom.GetImplicitValence())\n",
    "    print(\"GetAtomicNum\", atom.GetAtomicNum())\n",
    "    print(\"GetTotalNumHs\", atom.GetTotalNumHs())\n",
    "    print(\"GetNumImplicitHs\", atom.GetNumImplicitHs())\n",
    "    print(\"GetNeighbors\", atom.GetNeighbors())\n",
    "    print(\"GetNumExplicitHs\", atom.GetNumExplicitHs())\n",
    "    print(\"GetTotalDegree\", atom.GetTotalDegree())\n",
    "    print(\"GetTotalNumHs\", atom.GetTotalNumHs())\n",
    "    print(\"GetTotalValence\", atom.GetTotalValence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class QM9Dataset(Dataset):\n",
    "\n",
    "#     def __init__(self, file_name):\n",
    "#         file_out = pd.read_csv(file_name)\n",
    "#         x = file_out.iloc[:, 5]\n",
    "#         y = file_out.iloc[:, 10]\n",
    "\n",
    "#         self.X_train = torch.tensor(x)\n",
    "#         self.y_train = torch.tensor(y)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.y_train)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.X_train[idx], self.y_train[idx]\n",
    "\n",
    "dataset = pd.read_csv(\"./datasets/qm9.csv\")\n",
    "\n",
    "# Normalize targets to mean = 0 and std = 1.\n",
    "mean = dataset['g298'].mean()\n",
    "std = dataset['g298'].std()\n",
    "dataset['g298'] = (dataset['g298'] - mean) / std\n",
    "# mean, std = mean[:, 'g298'].item(), std[:, 'g298'].item()\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "dataset = shuffle(dataset)\n",
    "train_dataset = dataset[20000:70000]\n",
    "val_dataset = dataset[:10000]\n",
    "test_dataset = dataset[10000:20000]\n",
    "# train_dataset = dataset[:5000]\n",
    "# val_dataset = dataset[5000:7000]\n",
    "# test_dataset = dataset[7000:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECFPFeaturizer(Featurizer):\n",
    "    def __init__(self, y_column, radius=2, length=1024, **kwargs):\n",
    "        self.radius = radius\n",
    "        self.length = length\n",
    "        super().__init__(y_column, **kwargs)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        fingerprints = []\n",
    "        labels = []\n",
    "        for i, row in df.iterrows():\n",
    "            y = row[self.y_column]\n",
    "            smiles = row[self.smiles_col]\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, self.radius, nBits=self.length)\n",
    "            fingerprints.append(fp)\n",
    "            labels.append(y)\n",
    "        fingerprints = np.array(fingerprints)\n",
    "        labels = np.array(labels)\n",
    "        return fingerprints, labels\n",
    "\n",
    "rmse = Evaluator(name = 'MAE')\n",
    "\n",
    "featurizer = ECFPFeaturizer(y_column='g298', smiles_col=\"smiles\")\n",
    "X_train, y_train = featurizer(train_dataset)\n",
    "X_valid, y_valid = featurizer(val_dataset)\n",
    "X_test, y_test = featurizer(test_dataset)\n",
    "\n",
    "featurizer = GraphFeaturizer('g298', smiles_col=\"smiles\")\n",
    "\n",
    "graph = featurizer(test_dataset.iloc[:1], defaultRepresentation)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as GraphDataLoader\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader1 = GraphDataLoader(featurizer(train_dataset, representation1), batch_size=batch_size, shuffle=True)\n",
    "valid_loader1 = GraphDataLoader(featurizer(val_dataset, representation1), batch_size=batch_size)\n",
    "test_loader1 = GraphDataLoader(featurizer(test_dataset, representation1), batch_size=batch_size)\n",
    "\n",
    "train_loader10 = GraphDataLoader(featurizer(train_dataset, representation10), batch_size=batch_size, shuffle=True)\n",
    "valid_loader10 = GraphDataLoader(featurizer(val_dataset, representation10), batch_size=batch_size)\n",
    "test_loader10 = GraphDataLoader(featurizer(test_dataset, representation10), batch_size=batch_size)\n",
    "\n",
    "train_loader = GraphDataLoader(featurizer(train_dataset, representationAll), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = GraphDataLoader(featurizer(val_dataset, representationAll), batch_size=batch_size)\n",
    "test_loader = GraphDataLoader(featurizer(test_dataset, representationAll), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GINConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool as gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warstwa attention pooling\n",
    "class MyAttentionModule3(torch.nn.Module): # zakladamy ze atom ma 49 featerow\n",
    "    def __init__(self, groupFeatures=1):\n",
    "        super().__init__()\n",
    "        self.groupFeatures = groupFeatures\n",
    "        self.gates = torch.nn.ModuleDict({ # do wyliczenia atencji dla kazdej grupy cech - jest ich 9\n",
    "            'AtomicNum': GCNConv(12, 1),\n",
    "            'Degree': GCNConv(6, 1),\n",
    "            'TotalNumHs': GCNConv(5, 1),\n",
    "            'ImplicitValence': GCNConv(6, 1),\n",
    "            'Hybridization': GCNConv(5, 1),\n",
    "            'FormalCharge': GCNConv(1, 1),\n",
    "            'IsInRing': GCNConv(1, 1),\n",
    "            'IsAromatic': GCNConv(1, 1),\n",
    "            'NumRadicalElectrons': GCNConv(1, 1)\n",
    "        })\n",
    "        \n",
    "        self.feats = torch.nn.ModuleDict({ # do transformacji grupy cech w wektor, na razie dziala tylko dla groupFeatures=1\n",
    "            'AtomicNum': torch.nn.Linear(12, groupFeatures),\n",
    "            'Degree': torch.nn.Linear(6, groupFeatures),\n",
    "            'TotalNumHs': torch.nn.Linear(5, groupFeatures),\n",
    "            'ImplicitValence': torch.nn.Linear(6, groupFeatures),\n",
    "            'Hybridization': torch.nn.Linear(5, groupFeatures),\n",
    "            'FormalCharge': torch.nn.Linear(1, groupFeatures),\n",
    "            'IsInRing': torch.nn.Linear(1, groupFeatures),\n",
    "            'IsAromatic': torch.nn.Linear(1, groupFeatures),\n",
    "            'NumRadicalElectrons': torch.nn.Linear(1, groupFeatures)\n",
    "        })\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        gates = []\n",
    "        gates.append(self.gates['AtomicNum'](x[:,0:12], edge_index))\n",
    "        gates.append(self.gates['Degree'](x[:,12:18], edge_index))\n",
    "        gates.append(self.gates['TotalNumHs'](x[:,18:23], edge_index))\n",
    "        gates.append(self.gates['ImplicitValence'](x[:,23:29], edge_index))\n",
    "        gates.append(self.gates['Hybridization'](x[:,29:34], edge_index))\n",
    "        gates.append(self.gates['FormalCharge'](x[:,34:35], edge_index))\n",
    "        gates.append(self.gates['IsInRing'](x[:,35:36], edge_index))\n",
    "        gates.append(self.gates['IsAromatic'](x[:,36:37], edge_index))\n",
    "        gates.append(self.gates['NumRadicalElectrons'](x[:,37:38], edge_index))\n",
    "        logits = torch.cat(gates, dim=-1)\n",
    "        attention = torch.softmax(logits, dim=-1).unsqueeze(-1)\n",
    "        \n",
    "        subgroups = []\n",
    "        subgroups.append(self.feats['AtomicNum'](x[:,0:12]) * attention[:,0])\n",
    "        subgroups.append(self.feats['Degree'](x[:,12:18]) * attention[:,1])\n",
    "        subgroups.append(self.feats['TotalNumHs'](x[:,18:23]) * attention[:,2])\n",
    "        subgroups.append(self.feats['ImplicitValence'](x[:,23:29]) * attention[:,3])\n",
    "        subgroups.append(self.feats['Hybridization'](x[:,29:34]) * attention[:,4])\n",
    "        subgroups.append(self.feats['FormalCharge'](x[:,34:35]) * attention[:,5])\n",
    "        subgroups.append(self.feats['IsInRing'](x[:,35:36]) * attention[:,6])\n",
    "        subgroups.append(self.feats['IsAromatic'](x[:,36:37]) * attention[:,7])\n",
    "        subgroups.append(self.feats['NumRadicalElectrons'](x[:,37:38]) * attention[:,8])\n",
    "        x = torch.stack(subgroups, dim=-2)\n",
    "        x = torch.sum(x, dim=-2)\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attSequential(n_feats):\n",
    "    return torch.nn.Sequential(torch.nn.Linear(n_feats, 1),\n",
    "                       torch.nn.BatchNorm1d(1), torch.nn.ReLU(),\n",
    "                       torch.nn.Linear(1, 1), torch.nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warstwa attention pooling\n",
    "class MyAttentionModule4(torch.nn.Module): # zakladamy ze atom ma 49 featerow\n",
    "    def __init__(self, groupFeatures=1):\n",
    "        super().__init__()\n",
    "        self.groupFeatures = groupFeatures\n",
    "        self.gates = torch.nn.ModuleDict({ # do wyliczenia atencji dla kazdej grupy cech - jest ich 9\n",
    "            'AtomicNum': GINConv(attSequential(12), train_eps=True),\n",
    "            'Degree': GINConv(attSequential(6), train_eps=True),\n",
    "            'TotalNumHs': GINConv(attSequential(5), train_eps=True),\n",
    "            'ImplicitValence': GINConv(attSequential(6), train_eps=True),\n",
    "            'Hybridization': GINConv(attSequential(5), train_eps=True),\n",
    "            'FormalCharge': GINConv(attSequential(1), train_eps=True),\n",
    "            'IsInRing': GINConv(attSequential(1), train_eps=True),\n",
    "            'IsAromatic': GINConv(attSequential(1), train_eps=True),\n",
    "            'NumRadicalElectrons': GINConv(attSequential(1), train_eps=True)\n",
    "        })\n",
    "        \n",
    "        self.feats = torch.nn.ModuleDict({ # do transformacji grupy cech w wektor, na razie dziala tylko dla groupFeatures=1\n",
    "            'AtomicNum': torch.nn.Linear(12, groupFeatures),\n",
    "            'Degree': torch.nn.Linear(6, groupFeatures),\n",
    "            'TotalNumHs': torch.nn.Linear(5, groupFeatures),\n",
    "            'ImplicitValence': torch.nn.Linear(6, groupFeatures),\n",
    "            'Hybridization': torch.nn.Linear(5, groupFeatures),\n",
    "            'FormalCharge': torch.nn.Linear(1, groupFeatures),\n",
    "            'IsInRing': torch.nn.Linear(1, groupFeatures),\n",
    "            'IsAromatic': torch.nn.Linear(1, groupFeatures),\n",
    "            'NumRadicalElectrons': torch.nn.Linear(1, groupFeatures)\n",
    "        })\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        gates = []\n",
    "        gates.append(self.gates['AtomicNum'](x[:,0:12], edge_index))\n",
    "        gates.append(self.gates['Degree'](x[:,12:18], edge_index))\n",
    "        gates.append(self.gates['TotalNumHs'](x[:,18:23], edge_index))\n",
    "        gates.append(self.gates['ImplicitValence'](x[:,23:29], edge_index))\n",
    "        gates.append(self.gates['Hybridization'](x[:,29:34], edge_index))\n",
    "        gates.append(self.gates['FormalCharge'](x[:,34:35], edge_index))\n",
    "        gates.append(self.gates['IsInRing'](x[:,35:36], edge_index))\n",
    "        gates.append(self.gates['IsAromatic'](x[:,36:37], edge_index))\n",
    "        gates.append(self.gates['NumRadicalElectrons'](x[:,37:38], edge_index))\n",
    "        logits = torch.cat(gates, dim=-1)\n",
    "        attention = torch.softmax(logits, dim=-1).unsqueeze(-1)\n",
    "        \n",
    "        subgroups = []\n",
    "        subgroups.append(self.feats['AtomicNum'](x[:,0:12]) * attention[:,0])\n",
    "        subgroups.append(self.feats['Degree'](x[:,12:18]) * attention[:,1])\n",
    "        subgroups.append(self.feats['TotalNumHs'](x[:,18:23]) * attention[:,2])\n",
    "        subgroups.append(self.feats['ImplicitValence'](x[:,23:29]) * attention[:,3])\n",
    "        subgroups.append(self.feats['Hybridization'](x[:,29:34]) * attention[:,4])\n",
    "        subgroups.append(self.feats['FormalCharge'](x[:,34:35]) * attention[:,5])\n",
    "        subgroups.append(self.feats['IsInRing'](x[:,35:36]) * attention[:,6])\n",
    "        subgroups.append(self.feats['IsAromatic'](x[:,36:37]) * attention[:,7])\n",
    "        subgroups.append(self.feats['NumRadicalElectrons'](x[:,37:38]) * attention[:,8])\n",
    "        x = torch.stack(subgroups, dim=-2)\n",
    "        x = torch.sum(x, dim=-2)\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralNetwork(torch.nn.Module):  # TODO: assign hyperparameters to attributes and define the forward pass\n",
    "    def __init__(self, hidden_size, n_convs=3, my_layer=None, features_after_layer=26, n_features=49, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.myAttentionModule = my_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        convs = torch.nn.ModuleList()\n",
    "        convs.append(GCNConv(features_after_layer, hidden_size))\n",
    "        for i in range(1, n_convs):\n",
    "            convs.append(GCNConv(hidden_size, hidden_size))\n",
    "        self.convs = convs\n",
    "        self.linear = torch.nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        att = None\n",
    "        if self.myAttentionModule is not None:\n",
    "            x, att = self.myAttentionModule(x, edge_index, batch)\n",
    "        for i in range(0, len(self.convs)-1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = x.relu()\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        \n",
    "        x = gap(x, batch)\n",
    "        \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        out = self.linear(x)\n",
    "\n",
    "        return out, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, epochs=20, learning_rate = 0.01):\n",
    "    model.train()\n",
    "    \n",
    "    # training loop\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate) # TODO: define an optimizer\n",
    "    loss_fn = torch.nn.MSELoss()  # TODO: define a loss function\n",
    "    for epoch in trange(1, epochs + 1, leave=False):\n",
    "        for data in tqdm(train_loader, leave=False):\n",
    "            x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "            model.zero_grad()\n",
    "            preds, att = model(x, edge_index, batch)\n",
    "            loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            # print(\"==============\")\n",
    "            # for par in model.myAttentionModule.parameters():\n",
    "            #     print(par)\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, test_loader):\n",
    "    # evaluation loop\n",
    "    preds_batches = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            \n",
    "            preds, att = model(x, edge_index, batch)\n",
    "            preds_batches.append(preds.cpu().detach().numpy())\n",
    "    preds = np.concatenate(preds_batches)\n",
    "    return preds, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best(model, train_loader, valid_loader, epochs=20, learning_rate = 0.01, saveImg=False, title=\"\"):\n",
    "    model.train()\n",
    "\n",
    "    torch.save(model, \"train.pth\")\n",
    "    best_val = 1000000\n",
    "    \n",
    "    # training loop\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate) # TODO: define an optimizer\n",
    "    loss_fn = torch.nn.MSELoss()  # TODO: define a loss function\n",
    "    for epoch in trange(1, epochs + 1, leave=False):\n",
    "        # preds_batches = []\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "            model.zero_grad()\n",
    "            preds, att = model(x, edge_index, batch)\n",
    "            loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # evaluation loop\n",
    "        preds_batches = []\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "                preds, att = model(x, edge_index, batch)\n",
    "                loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "                preds_batches.append(preds.cpu().detach().numpy())\n",
    "        preds = np.concatenate(preds_batches)\n",
    "        mae = rmse(y_valid, preds.flatten())\n",
    "        if mae < best_val:\n",
    "            torch.save(model, \"train.pth\")\n",
    "            best_val = mae\n",
    "            print(best_val)\n",
    "\n",
    "    model = torch.load(\"train.pth\")\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(model, train_loader, valid_loader, test_loader, epochs=20, learning_rate = 0.01, saveImg=False, title=\"\"):\n",
    "    model.train()\n",
    "\n",
    "    torch.save(model, \"train.pth\")\n",
    "    best_val = 1000000\n",
    "    \n",
    "    # training loop\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate) # TODO: define an optimizer\n",
    "    loss_fn = torch.nn.MSELoss()  # TODO: define a loss function\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    for epoch in trange(1, epochs + 1, leave=False):\n",
    "        # preds_batches = []\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "            model.zero_grad()\n",
    "            preds, att = model(x, edge_index, batch)\n",
    "            loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "            # print(len(train_dataset))\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # preds_batches.append(preds.cpu().detach().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        # preds = np.concatenate(preds_batches)\n",
    "        # mae = rmse(y_train, preds.flatten())\n",
    "        # train_errors.append(mae)\n",
    "\n",
    "        # evaluation loop\n",
    "        preds_batches = []\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "                preds, att = model(x, edge_index, batch)\n",
    "                loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "                # print(len(train_dataset))\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                preds_batches.append(preds.cpu().detach().numpy())\n",
    "        epoch_loss = running_loss / len(valid_loader)\n",
    "        val_losses.append(epoch_loss)\n",
    "        preds = np.concatenate(preds_batches)\n",
    "        mae = rmse(y_valid, preds.flatten())\n",
    "        if mae < best_val:\n",
    "            torch.save(model, \"train.pth\")\n",
    "            best_val = mae\n",
    "            print(best_val)\n",
    "        val_errors.append(mae)\n",
    "\n",
    "    model = torch.load(\"train.pth\")\n",
    "    model.eval()\n",
    "\n",
    "    ##### visualize ########\n",
    "    plt.plot(train_losses, label='train_loss')\n",
    "    plt.plot(val_losses, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    if saveImg:\n",
    "        plt.savefig(title + \"_loss.png\")\n",
    "\n",
    "    # plt.plot(train_errors,label='train_errors')\n",
    "    plt.plot(val_errors, label='val_RMSE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    if saveImg:\n",
    "        plt.savefig(title + \"_val_error.png\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d4dd2195cc4842ae4b17a154a765bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5356048342149378\n",
      "0.498231150246177\n",
      "0.4530735670539323\n",
      "0.4436852798336852\n",
      "0.4208009805960195\n",
      "0.4142075631789886\n",
      "0.40792148212829143\n",
      "0.4049151005545354\n",
      "0.3941966575446578\n",
      "0.39333013087734264\n",
      "0.3858883516984422\n",
      "0.37235838269343785\n",
      "0.364354273799288\n",
      "0.36368514153628173\n",
      "0.36173526005494416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07810a16072044d6850e841f5643bf48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36\n"
     ]
    }
   ],
   "source": [
    "m =  GraphNeuralNetwork(512, n_convs=3, my_layer=MyAttentionModule4(35), features_after_layer=35)\n",
    "m = train_best(m, train_loader, valid_loader, epochs=100)\n",
    "predictions, att = predict(m, test_loader)\n",
    "rmse_score = rmse(y_test, predictions.flatten())\n",
    "print(\"{:.2f}\".format(rmse_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m.myAttentionModule.state_dict(), \"attention_pooling35_big.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldd23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
