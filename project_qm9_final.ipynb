{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tdc.single_pred.adme import ADME\n",
    "from tdc import Evaluator\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self, y_column, smiles_col='Drug', **kwargs):\n",
    "        self.y_column = y_column\n",
    "        self.smiles_col = smiles_col\n",
    "        self.__dict__.update(kwargs)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise ValueError(\"input {0} not in allowable set{1}:\".format(\n",
    "            x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "class GraphFeaturizer(Featurizer):\n",
    "    def __call__(self, df, getRepresentation):\n",
    "        graphs = []\n",
    "        labels = []\n",
    "        for i, row in df.iterrows():\n",
    "            y = row[self.y_column]\n",
    "            smiles = row[self.smiles_col]\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            \n",
    "            edges = []\n",
    "            for bond in mol.GetBonds():\n",
    "                begin = bond.GetBeginAtomIdx()\n",
    "                end = bond.GetEndAtomIdx()\n",
    "                edges.append((begin, end))  # TODO: Add edges in both directions\n",
    "            edges = np.array(edges)\n",
    "            \n",
    "            nodes = []\n",
    "            for atom in mol.GetAtoms():\n",
    "                # print(atom.GetAtomicNum(), atom.GetNumImplicitHs(), atom.GetTotalNumHs(), atom.GetSymbol(), atom.GetNumExplicitHs(), atom.GetTotalValence())\n",
    "                results = getRepresentation(atom)\n",
    "                # print(results)\n",
    "                nodes.append(results)\n",
    "            nodes = np.array(nodes)\n",
    "            \n",
    "            graphs.append((nodes, edges.T))\n",
    "            labels.append(y)\n",
    "        labels = np.array(labels)\n",
    "        return [Data(\n",
    "            x=torch.FloatTensor(x), \n",
    "            edge_index=torch.LongTensor(edge_index), \n",
    "            y=torch.FloatTensor([y])\n",
    "        ) for ((x, edge_index), y) in zip(graphs, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultRepresentation(atom):\n",
    "    return one_of_k_encoding_unk(atom.GetAtomicNum(), range(11)) + one_of_k_encoding(\n",
    "                    atom.GetDegree(), range(11)\n",
    "                ) + one_of_k_encoding_unk(\n",
    "                    atom.GetImplicitValence(), range(11)\n",
    "                ) + [atom.GetIsAromatic()] + one_of_k_encoding_unk(\n",
    "                    atom.GetTotalNumHs(), range(11)\n",
    "                ) + [atom.GetNumImplicitHs(), atom.GetFormalCharge(), atom.GetNumRadicalElectrons(), atom.IsInRing()] # TODO: Add atom features as a list, you can use one_of_k_encodings defined above\n",
    "\n",
    "def representation1(atom):\n",
    "    return one_of_k_encoding_unk(atom.GetAtomicNum(), range(12)) + one_of_k_encoding_unk(\n",
    "                    atom.GetDegree(), range(6)) + one_of_k_encoding_unk(\n",
    "                    atom.GetTotalNumHs(), range(5)\n",
    "                ) + [atom.GetFormalCharge(), atom.IsInRing(), atom.GetIsAromatic()]\n",
    "\n",
    "def representation10(atom):\n",
    "    return one_of_k_encoding_unk(atom.GetAtomicNum(), range(12)) + one_of_k_encoding_unk(\n",
    "                    atom.GetDegree(), range(6)) + one_of_k_encoding_unk(\n",
    "                    atom.GetTotalNumHs(), range(5)\n",
    "                ) + [atom.IsInRing(), atom.GetIsAromatic()]\n",
    "\n",
    "def representationAll(atom):\n",
    "    return one_of_k_encoding_unk(atom.GetAtomicNum(), range(12)) + one_of_k_encoding_unk(\n",
    "                    atom.GetDegree(), range(6)) + one_of_k_encoding_unk(\n",
    "                    atom.GetTotalNumHs(), range(5)) + one_of_k_encoding_unk(\n",
    "                    atom.GetImplicitValence(), range(6))  + one_of_k_encoding_unk(\n",
    "                    atom.GetHybridization(),\n",
    "                    [\n",
    "                        Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
    "                        Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D,\n",
    "                        Chem.rdchem.HybridizationType.SP3D2\n",
    "                    ]\n",
    "                ) + [atom.GetFormalCharge(), atom.IsInRing(), atom.GetIsAromatic()\n",
    "                     ] + [atom.GetNumRadicalElectrons()]\n",
    "\n",
    "def printProperties(atom):\n",
    "    print(\"=========\")\n",
    "    print(\"GetDegree\", atom.GetDegree())\n",
    "    print(\"GetImplicitValence\", atom.GetImplicitValence())\n",
    "    print(\"GetAtomicNum\", atom.GetAtomicNum())\n",
    "    print(\"GetTotalNumHs\", atom.GetTotalNumHs())\n",
    "    print(\"GetNumImplicitHs\", atom.GetNumImplicitHs())\n",
    "    print(\"GetNeighbors\", atom.GetNeighbors())\n",
    "    print(\"GetNumExplicitHs\", atom.GetNumExplicitHs())\n",
    "    print(\"GetTotalDegree\", atom.GetTotalDegree())\n",
    "    print(\"GetTotalNumHs\", atom.GetTotalNumHs())\n",
    "    print(\"GetTotalValence\", atom.GetTotalValence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class QM9Dataset(Dataset):\n",
    "\n",
    "#     def __init__(self, file_name):\n",
    "#         file_out = pd.read_csv(file_name)\n",
    "#         x = file_out.iloc[:, 5]\n",
    "#         y = file_out.iloc[:, 10]\n",
    "\n",
    "#         self.X_train = torch.tensor(x)\n",
    "#         self.y_train = torch.tensor(y)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.y_train)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.X_train[idx], self.y_train[idx]\n",
    "\n",
    "dataset = pd.read_csv(\"./datasets/qm9.csv\")\n",
    "\n",
    "# Normalize targets to mean = 0 and std = 1.\n",
    "mean = dataset['g298'].mean()\n",
    "std = dataset['g298'].std()\n",
    "dataset['g298'] = (dataset['g298'] - mean) / std\n",
    "# mean, std = mean[:, 'g298'].item(), std[:, 'g298'].item()\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "dataset = shuffle(dataset)\n",
    "# train_dataset = dataset[20000:70000]\n",
    "# val_dataset = dataset[:10000]\n",
    "# test_dataset = dataset[10000:20000]\n",
    "train_dataset = dataset[:5000]\n",
    "val_dataset = dataset[5000:7000]\n",
    "test_dataset = dataset[7000:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECFPFeaturizer(Featurizer):\n",
    "    def __init__(self, y_column, radius=2, length=1024, **kwargs):\n",
    "        self.radius = radius\n",
    "        self.length = length\n",
    "        super().__init__(y_column, **kwargs)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        fingerprints = []\n",
    "        labels = []\n",
    "        for i, row in df.iterrows():\n",
    "            y = row[self.y_column]\n",
    "            smiles = row[self.smiles_col]\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, self.radius, nBits=self.length)\n",
    "            fingerprints.append(fp)\n",
    "            labels.append(y)\n",
    "        fingerprints = np.array(fingerprints)\n",
    "        labels = np.array(labels)\n",
    "        return fingerprints, labels\n",
    "\n",
    "rmse = Evaluator(name = 'MAE')\n",
    "\n",
    "featurizer = ECFPFeaturizer(y_column='g298', smiles_col=\"smiles\")\n",
    "X_train, y_train = featurizer(train_dataset)\n",
    "X_valid, y_valid = featurizer(val_dataset)\n",
    "X_test, y_test = featurizer(test_dataset)\n",
    "\n",
    "featurizer = GraphFeaturizer('g298', smiles_col=\"smiles\")\n",
    "\n",
    "graph = featurizer(test_dataset.iloc[:1], defaultRepresentation)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as GraphDataLoader\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader1 = GraphDataLoader(featurizer(train_dataset, representation1), batch_size=batch_size, shuffle=True)\n",
    "valid_loader1 = GraphDataLoader(featurizer(val_dataset, representation1), batch_size=batch_size)\n",
    "test_loader1 = GraphDataLoader(featurizer(test_dataset, representation1), batch_size=batch_size)\n",
    "\n",
    "train_loader10 = GraphDataLoader(featurizer(train_dataset, representation10), batch_size=batch_size, shuffle=True)\n",
    "valid_loader10 = GraphDataLoader(featurizer(val_dataset, representation10), batch_size=batch_size)\n",
    "test_loader10 = GraphDataLoader(featurizer(test_dataset, representation10), batch_size=batch_size)\n",
    "\n",
    "train_loader = GraphDataLoader(featurizer(train_dataset, representationAll), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = GraphDataLoader(featurizer(val_dataset, representationAll), batch_size=batch_size)\n",
    "test_loader = GraphDataLoader(featurizer(test_dataset, representationAll), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GINConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool as gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warstwa attention pooling\n",
    "class MyAttentionModule3(torch.nn.Module): # zakladamy ze atom ma 49 featerow\n",
    "    def __init__(self, groupFeatures=1):\n",
    "        super().__init__()\n",
    "        self.groupFeatures = groupFeatures\n",
    "        self.gates = torch.nn.ModuleDict({ # do wyliczenia atencji dla kazdej grupy cech - jest ich 9\n",
    "            'AtomicNum': GCNConv(12, 1),\n",
    "            'Degree': GCNConv(6, 1),\n",
    "            'TotalNumHs': GCNConv(5, 1),\n",
    "            'ImplicitValence': GCNConv(6, 1),\n",
    "            'Hybridization': GCNConv(5, 1),\n",
    "            'FormalCharge': GCNConv(1, 1),\n",
    "            'IsInRing': GCNConv(1, 1),\n",
    "            'IsAromatic': GCNConv(1, 1),\n",
    "            'NumRadicalElectrons': GCNConv(1, 1)\n",
    "        })\n",
    "        \n",
    "        self.feats = torch.nn.ModuleDict({ # do transformacji grupy cech w wektor, na razie dziala tylko dla groupFeatures=1\n",
    "            'AtomicNum': torch.nn.Linear(12, groupFeatures),\n",
    "            'Degree': torch.nn.Linear(6, groupFeatures),\n",
    "            'TotalNumHs': torch.nn.Linear(5, groupFeatures),\n",
    "            'ImplicitValence': torch.nn.Linear(6, groupFeatures),\n",
    "            'Hybridization': torch.nn.Linear(5, groupFeatures),\n",
    "            'FormalCharge': torch.nn.Linear(1, groupFeatures),\n",
    "            'IsInRing': torch.nn.Linear(1, groupFeatures),\n",
    "            'IsAromatic': torch.nn.Linear(1, groupFeatures),\n",
    "            'NumRadicalElectrons': torch.nn.Linear(1, groupFeatures)\n",
    "        })\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        gates = []\n",
    "        gates.append(self.gates['AtomicNum'](x[:,0:12], edge_index))\n",
    "        gates.append(self.gates['Degree'](x[:,12:18], edge_index))\n",
    "        gates.append(self.gates['TotalNumHs'](x[:,18:23], edge_index))\n",
    "        gates.append(self.gates['ImplicitValence'](x[:,23:29], edge_index))\n",
    "        gates.append(self.gates['Hybridization'](x[:,29:34], edge_index))\n",
    "        gates.append(self.gates['FormalCharge'](x[:,34:35], edge_index))\n",
    "        gates.append(self.gates['IsInRing'](x[:,35:36], edge_index))\n",
    "        gates.append(self.gates['IsAromatic'](x[:,36:37], edge_index))\n",
    "        gates.append(self.gates['NumRadicalElectrons'](x[:,37:38], edge_index))\n",
    "        logits = torch.cat(gates, dim=-1)\n",
    "        attention = torch.softmax(logits, dim=-1).unsqueeze(-1)\n",
    "        \n",
    "        subgroups = []\n",
    "        subgroups.append(self.feats['AtomicNum'](x[:,0:12]) * attention[:,0])\n",
    "        subgroups.append(self.feats['Degree'](x[:,12:18]) * attention[:,1])\n",
    "        subgroups.append(self.feats['TotalNumHs'](x[:,18:23]) * attention[:,2])\n",
    "        subgroups.append(self.feats['ImplicitValence'](x[:,23:29]) * attention[:,3])\n",
    "        subgroups.append(self.feats['Hybridization'](x[:,29:34]) * attention[:,4])\n",
    "        subgroups.append(self.feats['FormalCharge'](x[:,34:35]) * attention[:,5])\n",
    "        subgroups.append(self.feats['IsInRing'](x[:,35:36]) * attention[:,6])\n",
    "        subgroups.append(self.feats['IsAromatic'](x[:,36:37]) * attention[:,7])\n",
    "        subgroups.append(self.feats['NumRadicalElectrons'](x[:,37:38]) * attention[:,8])\n",
    "        x = torch.stack(subgroups, dim=-2)\n",
    "        x = torch.sum(x, dim=-2)\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attSequential(n_feats):\n",
    "    return torch.nn.Sequential(torch.nn.Linear(n_feats, 1),\n",
    "                       torch.nn.BatchNorm1d(1), torch.nn.ReLU(),\n",
    "                       torch.nn.Linear(1, 1), torch.nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warstwa attention pooling\n",
    "class MyAttentionModule4(torch.nn.Module): # zakladamy ze atom ma 49 featerow\n",
    "    def __init__(self, groupFeatures=1):\n",
    "        super().__init__()\n",
    "        self.groupFeatures = groupFeatures\n",
    "        self.gates = torch.nn.ModuleDict({ # do wyliczenia atencji dla kazdej grupy cech - jest ich 9\n",
    "            'AtomicNum': GINConv(attSequential(12), train_eps=True),\n",
    "            'Degree': GINConv(attSequential(6), train_eps=True),\n",
    "            'TotalNumHs': GINConv(attSequential(5), train_eps=True),\n",
    "            'ImplicitValence': GINConv(attSequential(6), train_eps=True),\n",
    "            'Hybridization': GINConv(attSequential(5), train_eps=True),\n",
    "            'FormalCharge': GINConv(attSequential(1), train_eps=True),\n",
    "            'IsInRing': GINConv(attSequential(1), train_eps=True),\n",
    "            'IsAromatic': GINConv(attSequential(1), train_eps=True),\n",
    "            'NumRadicalElectrons': GINConv(attSequential(1), train_eps=True)\n",
    "        })\n",
    "        \n",
    "        self.feats = torch.nn.ModuleDict({ # do transformacji grupy cech w wektor, na razie dziala tylko dla groupFeatures=1\n",
    "            'AtomicNum': torch.nn.Linear(12, groupFeatures),\n",
    "            'Degree': torch.nn.Linear(6, groupFeatures),\n",
    "            'TotalNumHs': torch.nn.Linear(5, groupFeatures),\n",
    "            'ImplicitValence': torch.nn.Linear(6, groupFeatures),\n",
    "            'Hybridization': torch.nn.Linear(5, groupFeatures),\n",
    "            'FormalCharge': torch.nn.Linear(1, groupFeatures),\n",
    "            'IsInRing': torch.nn.Linear(1, groupFeatures),\n",
    "            'IsAromatic': torch.nn.Linear(1, groupFeatures),\n",
    "            'NumRadicalElectrons': torch.nn.Linear(1, groupFeatures)\n",
    "        })\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        gates = []\n",
    "        gates.append(self.gates['AtomicNum'](x[:,0:12], edge_index))\n",
    "        gates.append(self.gates['Degree'](x[:,12:18], edge_index))\n",
    "        gates.append(self.gates['TotalNumHs'](x[:,18:23], edge_index))\n",
    "        gates.append(self.gates['ImplicitValence'](x[:,23:29], edge_index))\n",
    "        gates.append(self.gates['Hybridization'](x[:,29:34], edge_index))\n",
    "        gates.append(self.gates['FormalCharge'](x[:,34:35], edge_index))\n",
    "        gates.append(self.gates['IsInRing'](x[:,35:36], edge_index))\n",
    "        gates.append(self.gates['IsAromatic'](x[:,36:37], edge_index))\n",
    "        gates.append(self.gates['NumRadicalElectrons'](x[:,37:38], edge_index))\n",
    "        logits = torch.cat(gates, dim=-1)\n",
    "        attention = torch.softmax(logits, dim=-1).unsqueeze(-1)\n",
    "        \n",
    "        subgroups = []\n",
    "        subgroups.append(self.feats['AtomicNum'](x[:,0:12]) * attention[:,0])\n",
    "        subgroups.append(self.feats['Degree'](x[:,12:18]) * attention[:,1])\n",
    "        subgroups.append(self.feats['TotalNumHs'](x[:,18:23]) * attention[:,2])\n",
    "        subgroups.append(self.feats['ImplicitValence'](x[:,23:29]) * attention[:,3])\n",
    "        subgroups.append(self.feats['Hybridization'](x[:,29:34]) * attention[:,4])\n",
    "        subgroups.append(self.feats['FormalCharge'](x[:,34:35]) * attention[:,5])\n",
    "        subgroups.append(self.feats['IsInRing'](x[:,35:36]) * attention[:,6])\n",
    "        subgroups.append(self.feats['IsAromatic'](x[:,36:37]) * attention[:,7])\n",
    "        subgroups.append(self.feats['NumRadicalElectrons'](x[:,37:38]) * attention[:,8])\n",
    "        x = torch.stack(subgroups, dim=-2)\n",
    "        x = torch.sum(x, dim=-2)\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralNetwork(torch.nn.Module):  # TODO: assign hyperparameters to attributes and define the forward pass\n",
    "    def __init__(self, hidden_size, n_convs=3, my_layer=None, features_after_layer=26, n_features=49, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.myAttentionModule = my_layer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        convs = torch.nn.ModuleList()\n",
    "        convs.append(GCNConv(features_after_layer, hidden_size))\n",
    "        for i in range(1, n_convs):\n",
    "            convs.append(GCNConv(hidden_size, hidden_size))\n",
    "        self.convs = convs\n",
    "        self.linear = torch.nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        att = None\n",
    "        if self.myAttentionModule is not None:\n",
    "            x, att = self.myAttentionModule(x, edge_index, batch)\n",
    "        for i in range(0, len(self.convs)-1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = x.relu()\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        \n",
    "        x = gap(x, batch)\n",
    "        \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        out = self.linear(x)\n",
    "\n",
    "        return out, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, epochs=20, learning_rate = 0.01):\n",
    "    model.train()\n",
    "    \n",
    "    # training loop\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate) # TODO: define an optimizer\n",
    "    loss_fn = torch.nn.MSELoss()  # TODO: define a loss function\n",
    "    for epoch in trange(1, epochs + 1, leave=False):\n",
    "        for data in tqdm(train_loader, leave=False):\n",
    "            x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "            model.zero_grad()\n",
    "            preds, att = model(x, edge_index, batch)\n",
    "            loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            # print(\"==============\")\n",
    "            # for par in model.myAttentionModule.parameters():\n",
    "            #     print(par)\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, test_loader):\n",
    "    # evaluation loop\n",
    "    preds_batches = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            \n",
    "            preds, att = model(x, edge_index, batch)\n",
    "            preds_batches.append(preds.cpu().detach().numpy())\n",
    "    preds = np.concatenate(preds_batches)\n",
    "    return preds, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def train_best(model, train_loader, valid_loader, epochs=20, learning_rate = 0.01, saveImg=False, title=\"\"):\n",
    "    model.train()\n",
    "\n",
    "    best_state = deepcopy(model.state_dict())\n",
    "    best_val = 1000000\n",
    "    \n",
    "    # training loop\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate) # TODO: define an optimizer\n",
    "    loss_fn = torch.nn.MSELoss()  # TODO: define a loss function\n",
    "    for epoch in trange(1, epochs + 1, leave=False):\n",
    "        # preds_batches = []\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "            model.zero_grad()\n",
    "            preds, att = model(x, edge_index, batch)\n",
    "            loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # evaluation loop\n",
    "        preds_batches = []\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "                preds, att = model(x, edge_index, batch)\n",
    "                loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "                preds_batches.append(preds.cpu().detach().numpy())\n",
    "        preds = np.concatenate(preds_batches)\n",
    "        mae = rmse(y_valid, preds.flatten())\n",
    "        if mae < best_val:\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            best_val = mae\n",
    "            print(best_val)\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(model, train_loader, valid_loader, test_loader, epochs=20, learning_rate = 0.01, saveImg=False, title=\"\"):\n",
    "    model.train()\n",
    "\n",
    "    best_state = deepcopy(model.state_dict())\n",
    "    best_val = 1000000\n",
    "    \n",
    "    # training loop\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate) # TODO: define an optimizer\n",
    "    loss_fn = torch.nn.MSELoss()  # TODO: define a loss function\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    for epoch in trange(1, epochs + 1, leave=False):\n",
    "        # preds_batches = []\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "            model.zero_grad()\n",
    "            preds, att = model(x, edge_index, batch)\n",
    "            loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "            # print(len(train_dataset))\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # preds_batches.append(preds.cpu().detach().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        # preds = np.concatenate(preds_batches)\n",
    "        # mae = rmse(y_train, preds.flatten())\n",
    "        # train_errors.append(mae)\n",
    "\n",
    "        # evaluation loop\n",
    "        preds_batches = []\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "                preds, att = model(x, edge_index, batch)\n",
    "                loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "                # print(len(train_dataset))\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                preds_batches.append(preds.cpu().detach().numpy())\n",
    "        epoch_loss = running_loss / len(valid_loader)\n",
    "        val_losses.append(epoch_loss)\n",
    "        preds = np.concatenate(preds_batches)\n",
    "        mae = rmse(y_valid, preds.flatten())\n",
    "        if mae < best_val:\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            best_val = mae\n",
    "            print(best_val)\n",
    "        val_errors.append(mae)\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "    ##### visualize ########\n",
    "    plt.plot(train_losses, label='train_loss')\n",
    "    plt.plot(val_losses, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    if saveImg:\n",
    "        plt.savefig(title + \"_loss.png\")\n",
    "\n",
    "    # plt.plot(train_errors,label='train_errors')\n",
    "    plt.plot(val_errors, label='val_RMSE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    if saveImg:\n",
    "        plt.savefig(title + \"_val_error.png\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78\n"
     ]
    }
   ],
   "source": [
    "m =  GraphNeuralNetwork(512, n_convs=3, features_after_layer=25)\n",
    "predictions, att = predict(m, test_loader10)\n",
    "rmse_score = rmse(y_test, predictions.flatten())\n",
    "print(\"{:.2f}\".format(rmse_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63941efb1d914dff9bda29a63a555777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6141783793969974\n",
      "0.5633457925429605\n",
      "0.5601623081095508\n",
      "0.5495416510668535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m######################## wizualizacje #################################################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m m \u001b[39m=\u001b[39m GraphNeuralNetwork(\u001b[39m512\u001b[39m, my_layer\u001b[39m=\u001b[39mMyAttentionModule4(\u001b[39m3\u001b[39m), features_after_layer\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m m \u001b[39m=\u001b[39m visualize(m, train_loader, valid_loader, test_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, saveImg\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, title\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMyAttentionModule4(3)_qm9\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m torch\u001b[39m.\u001b[39msave(m\u001b[39m.\u001b[39mmyAttentionModule\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mattention_pooling3.pth\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m m \u001b[39m=\u001b[39m GraphNeuralNetwork(\u001b[39m512\u001b[39m, my_layer\u001b[39m=\u001b[39mMyAttentionModule4(\u001b[39m35\u001b[39m), features_after_layer\u001b[39m=\u001b[39m\u001b[39m35\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 20\u001b[0m, in \u001b[0;36mvisualize\u001b[1;34m(model, train_loader, valid_loader, test_loader, epochs, learning_rate, saveImg, title)\u001b[0m\n\u001b[0;32m     18\u001b[0m x, edge_index, batch, y \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index, data\u001b[39m.\u001b[39mbatch, data\u001b[39m.\u001b[39my\n\u001b[0;32m     19\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m preds, att \u001b[39m=\u001b[39m model(x, edge_index, batch)\n\u001b[0;32m     21\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(preds, y\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m     22\u001b[0m \u001b[39m# print(len(train_dataset))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\its\\miniconda3\\envs\\mldd23\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m, in \u001b[0;36mGraphNeuralNetwork.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs[i](x, edge_index)\n\u001b[0;32m     20\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrelu()\n\u001b[1;32m---> 21\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvs[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](x, edge_index)\n\u001b[0;32m     23\u001b[0m x \u001b[39m=\u001b[39m gap(x, batch)\n\u001b[0;32m     25\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n",
      "File \u001b[1;32mc:\\Users\\its\\miniconda3\\envs\\mldd23\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\its\\miniconda3\\envs\\mldd23\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:210\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    208\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index\n\u001b[0;32m    209\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 210\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m gcn_norm(  \u001b[39m# yapf: disable\u001b[39;49;00m\n\u001b[0;32m    211\u001b[0m         edge_index, edge_weight, x\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim),\n\u001b[0;32m    212\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimproved, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_self_loops, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow, x\u001b[39m.\u001b[39;49mdtype)\n\u001b[0;32m    213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached:\n\u001b[0;32m    214\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index \u001b[39m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[1;32mc:\\Users\\its\\miniconda3\\envs\\mldd23\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:91\u001b[0m, in \u001b[0;36mgcn_norm\u001b[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[0;32m     88\u001b[0m num_nodes \u001b[39m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m add_self_loops:\n\u001b[1;32m---> 91\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m add_remaining_self_loops(\n\u001b[0;32m     92\u001b[0m         edge_index, edge_weight, fill_value, num_nodes)\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m edge_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     edge_weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((edge_index\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), ), dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m     96\u001b[0m                              device\u001b[39m=\u001b[39medge_index\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\its\\miniconda3\\envs\\mldd23\\lib\\site-packages\\torch_geometric\\utils\\loop.py:340\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[1;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Adds remaining self-loop :math:`(i,i) \\in \\mathcal{E}` to every node\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m:math:`i \\in \\mathcal{V}` in the graph given by :attr:`edge_index`.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39mIn case the graph is weighted or has multi-dimensional edge features\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39m    tensor([0.5000, 0.5000, 1.0000, 1.0000]))\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m N \u001b[39m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[1;32m--> 340\u001b[0m mask \u001b[39m=\u001b[39m edge_index[\u001b[39m0\u001b[39;49m] \u001b[39m!=\u001b[39;49m edge_index[\u001b[39m1\u001b[39;49m]\n\u001b[0;32m    342\u001b[0m loop_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, N, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39medge_index\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    343\u001b[0m loop_index \u001b[39m=\u001b[39m loop_index\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mrepeat(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################## wizualizacje #################################################\n",
    "m = GraphNeuralNetwork(512, my_layer=MyAttentionModule4(3), features_after_layer=3)\n",
    "m = visualize(m, train_loader, valid_loader, test_loader, epochs=100, saveImg=True, title=\"MyAttentionModule4(3)_qm9\")\n",
    "torch.save(m.myAttentionModule.state_dict(), \"attention_pooling3.pth\")\n",
    "\n",
    "m = GraphNeuralNetwork(512, my_layer=MyAttentionModule4(35), features_after_layer=35)\n",
    "m = visualize(m, train_loader, valid_loader, test_loader, epochs=100, saveImg=True, title=\"MyAttentionModule4(35)_qm9\")\n",
    "torch.save(m.myAttentionModule.state_dict(), \"attention_pooling35.pth\")\n",
    "\n",
    "m = GraphNeuralNetwork(512, my_layer=MyAttentionModule4(100), features_after_layer=100)\n",
    "m = visualize(m, train_loader, valid_loader, test_loader, epochs=100, saveImg=True, title=\"MyAttentionModule4(100)_qm9\")\n",
    "torch.save(m.myAttentionModule.state_dict(), \"attention_pooling100.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed996cc39266461093e7da48374c80e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5501999533014232\n",
      "0.5040687140639524\n",
      "0.49571092890067847\n",
      "0.4846211159043977\n",
      "0.48320055516095733\n",
      "0.4798495588766813\n",
      "0.47862055974973894\n",
      "0.46959427808645865\n",
      "0.4692307116427061\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cea21b4276490da71ee827e0cf5eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5118019648357927\n",
      "0.5108146163040406\n",
      "0.4840445827762349\n",
      "0.4742861989983277\n",
      "0.47330477481478156\n",
      "0.4681885657518951\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0de184412a48b0bc4f875ac3fc6f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.561036359623823\n",
      "0.4769177162116058\n",
      "0.4744529705127069\n",
      "0.40883556785632447\n",
      "0.3919386160856415\n",
      "0.38347617034513065\n",
      "0.37588342552404896\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0f34e17050454997ba04db6887667d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5742107988669302\n",
      "0.4906674313564381\n",
      "0.434849694428749\n",
      "0.4063036310486408\n",
      "0.38088680098015626\n",
      "0.3791061764394474\n",
      "0.375671778923798\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4046c38b3b824596a0c2de533afdb7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5648204590068437\n",
      "0.4204737493766412\n",
      "0.40843622716617833\n",
      "0.39246518593037233\n",
      "0.3902598677958982\n",
      "0.37078907308215087\n",
      "0.3595757179397779\n",
      "0.35346201845336206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114b9944e4e14bd9b75c2984b83b1a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5520287189781918\n",
      "0.48559857960289954\n",
      "0.47923544684714\n",
      "0.47886685720290395\n",
      "0.471933309270446\n",
      "0.46607840339390455\n",
      "0.46480330948283693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef688569e061478fa0cce5e234f1234f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5110998178408238\n",
      "0.4860796685755312\n",
      "0.47270051578897415\n",
      "0.46767733701163205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2341accce8d41a48369c2a44024cfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.580995371184952\n",
      "0.500531178153558\n",
      "0.48313098006812244\n",
      "0.4717063011754981\n",
      "0.43893967279513935\n",
      "0.43649096040999275\n",
      "0.42205237154106046\n",
      "0.4111093730560148\n",
      "0.4087300721747237\n",
      "0.39159351770083184\n",
      "0.3885609372455406\n",
      "0.3858369447002459\n",
      "0.3776937232135942\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a3851258b64d03993be155a6ef5048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5115080770676937\n",
      "0.4839151094237457\n",
      "0.47667585911141314\n",
      "0.4610168447248598\n",
      "0.45971754702866247\n",
      "0.4449372632458583\n",
      "0.43868778504774103\n",
      "0.4380334812758879\n",
      "0.42161911148377895\n",
      "0.42040963100370005\n",
      "0.4177185036045646\n",
      "0.4162841444610132\n",
      "0.4100220688922407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524c862c92af483c8101bf791efd5513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4971449339566476\n",
      "0.46939684863810033\n",
      "0.3999995704549018\n",
      "0.38963417552717733\n",
      "0.37576055820742543\n",
      "0.37317306945242273\n",
      "0.37162927551413566\n",
      "0.3705623131299011\n",
      "0.369619645832106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f60c6dc1ca74dc89fc2e1f6f2365088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5939083538699361\n",
      "0.5615732030683832\n",
      "0.5558171326819229\n",
      "0.5047088203492285\n",
      "0.4996396830875631\n",
      "0.46113020371174956\n",
      "0.45055473600158247\n",
      "0.4311138086792339\n",
      "0.4308139026254159\n",
      "0.42418769599034023\n",
      "0.41468181470194376\n",
      "0.4108372316430756\n",
      "0.40117436921112204\n",
      "0.394027935025768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f5b4ca2c2e4477b7320cd39385069f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6024437365507039\n",
      "0.5874033679263448\n",
      "0.5599956640625363\n",
      "0.548706039783326\n",
      "0.5459461065742708\n",
      "0.5351095562370498\n",
      "0.532330340531174\n",
      "0.5073445092172706\n",
      "0.4780029155425512\n",
      "0.4705713139339443\n",
      "0.46925587375914457\n",
      "0.4622008044281099\n",
      "0.4380974568998195\n",
      "0.4255670204326471\n",
      "0.4172136271366522\n",
      "0.4131157312551266\n",
      "0.412631130078639\n",
      "0.4057599442814407\n",
      "0.4017204481952684\n",
      "0.4003492571599886\n",
      "0.3942722603266621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657b5ac9f76d433bb1d81fbbb5a125fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6402671247952043\n",
      "0.6142171073451764\n",
      "0.5876454278940789\n",
      "0.5532898057821921\n",
      "0.5531518375183435\n",
      "0.5384364251581455\n",
      "0.5334754907506167\n",
      "0.5327773013030714\n",
      "0.5316496719074199\n",
      "0.5195250010786147\n",
      "0.5172499747546381\n",
      "0.5155615694760607\n",
      "0.4996217550690369\n",
      "0.482203989301975\n",
      "0.47989741909303657\n",
      "0.4673173154863694\n",
      "0.4589998236036089\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0beab232294744ac1b64a965998752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5990910163986459\n",
      "0.560650603171747\n",
      "0.5329261031420295\n",
      "0.5192210622399327\n",
      "0.5039859453464626\n",
      "0.4796484833384004\n",
      "0.47194156123973896\n",
      "0.40369999642342863\n",
      "0.39981257435271494\n",
      "0.39177203012138206\n",
      "0.375800431581988\n",
      "0.37310950642631874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151315ede10a41dc9ec4cddbfdc5e68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5961680073045709\n",
      "0.5464317482454847\n",
      "0.5360795353333796\n",
      "0.5241916930285079\n",
      "0.5215370248711109\n",
      "0.5135582275801003\n",
      "0.51198900564209\n",
      "0.4805410595677199\n",
      "0.4427752716958865\n",
      "0.4242470715817855\n",
      "0.4138198931521222\n",
      "0.3983854882043393\n",
      "0.39687253401010253\n",
      "0.3810191248581649\n",
      "0.3789726740280708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4ad5e5635a461599b1dca64ed6f452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6013737373952691\n",
      "0.5771409082717582\n",
      "0.5696834634191016\n",
      "0.5389842464076748\n",
      "0.49568055165774105\n",
      "0.4788574731403034\n",
      "0.4633257464534358\n",
      "0.4620427472834565\n",
      "0.4455838321973563\n",
      "0.431746147987735\n",
      "0.4303277774400391\n",
      "0.4252157013912176\n",
      "0.42393361418152226\n",
      "0.4234115601704591\n",
      "0.4174226800693867\n",
      "0.41124543255903345\n",
      "0.4085237609230134\n",
      "0.40821851432693457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0461492c07e641b48bffd802c616fdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5855291139264885\n",
      "0.5762779694885737\n",
      "0.5623906308164883\n",
      "0.5324334075134749\n",
      "0.5082324020746836\n",
      "0.49596007457916674\n",
      "0.49238105003498\n",
      "0.46853329893231593\n",
      "0.42858921839848313\n",
      "0.41299982583538164\n",
      "0.4027639917916573\n",
      "0.39330557101694147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6a77a662d84978a5016b09cd5992db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7700663665696368\n",
      "0.6258430699017373\n",
      "0.5480813684631621\n",
      "0.5246136135178515\n",
      "0.52333965167258\n",
      "0.5182618883311261\n",
      "0.5118764002443064\n",
      "0.5113094588617306\n",
      "0.4892476630757904\n",
      "0.48789822612558276\n",
      "0.4857481720232788\n",
      "0.4751653413753546\n",
      "0.46367547329697856\n",
      "0.45119743766910164\n",
      "0.4271310554046398\n",
      "0.41033696731509495\n",
      "0.40916602722142964\n",
      "0.3962517001854777\n",
      "0.38474713037150365\n",
      "0.3831188286186428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72f26b1ab7d46fbab389386b20ca76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.611284880529322\n",
      "0.5798016921132954\n",
      "0.565883679141672\n",
      "0.5413909704483308\n",
      "0.5325595203770777\n",
      "0.495290180387102\n",
      "0.46991538002175937\n",
      "0.43329269039001633\n",
      "0.4308263080636379\n",
      "0.4119763966868199\n",
      "0.3981382644776737\n",
      "0.38780293228608625\n",
      "0.3860493180065903\n",
      "0.38380125918291036\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38a58a5dc174cf98338d81168c79858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5984623931680215\n",
      "0.5850773963589374\n",
      "0.5459695930287695\n",
      "0.5339812214641233\n",
      "0.48530988756258764\n",
      "0.4232247841968625\n",
      "0.4131790364496137\n",
      "0.41304306628359727\n",
      "0.3975458125707353\n",
      "0.3964233141411874\n",
      "0.3884801657224965\n",
      "0.3869223800678591\n",
      "0.38329221121805396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7fe5a995644139a358569ee01d4910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6245888747546967\n",
      "0.6244082215837307\n",
      "0.6111743954891482\n",
      "0.5845686551527098\n",
      "0.5822581866345424\n",
      "0.5776427382063503\n",
      "0.5773658302516803\n",
      "0.5701176173809611\n",
      "0.5646273334223258\n",
      "0.5591248831534879\n",
      "0.558012464156884\n",
      "0.5500178118270967\n",
      "0.545211788864557\n",
      "0.5152461277135829\n",
      "0.5010758204089854\n",
      "0.4991763902654609\n",
      "0.4848839998184014\n",
      "0.48055700644152055\n",
      "0.4612927018018304\n",
      "0.455131507237876\n",
      "0.4511284425318283\n",
      "0.44842515259452376\n",
      "0.44513350927822115\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccdbabfc6d145ba9686154c1420aea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6247117070379029\n",
      "0.6142644878459667\n",
      "0.595417212792111\n",
      "0.5848691892958298\n",
      "0.5776846867037703\n",
      "0.577596518169868\n",
      "0.5574309936994306\n",
      "0.5566640070829169\n",
      "0.551759354738294\n",
      "0.5349935126873452\n",
      "0.5323323754216144\n",
      "0.5263573244682348\n",
      "0.5136803859961836\n",
      "0.4905481854376052\n",
      "0.4901522603276309\n",
      "0.4870854981845266\n",
      "0.4813231839752734\n",
      "0.461438423790292\n",
      "0.46082964199684817\n",
      "0.4604154504012818\n",
      "0.45559996216612086\n",
      "0.44283872965349647\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f42e000d02a4a0f9c7a18b62dd93b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7819804065579496\n",
      "0.7767797867340568\n",
      "0.774594113960471\n",
      "0.7745094114248571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234b19159d8e48ddbd8a5f36bd332d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6383309652584185\n",
      "0.6162205190964987\n",
      "0.5897108050863753\n",
      "0.5750126141373477\n",
      "0.5734331495541883\n",
      "0.5659146422179062\n",
      "0.5641528420170541\n",
      "0.5635419899128277\n",
      "0.5589319450224152\n",
      "0.5552536022318175\n",
      "0.5525612909485301\n",
      "0.5434841838195471\n",
      "0.5353358723289614\n",
      "0.5330759415650079\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4cee7c1f674cbcabe48052e351e74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6418846107341808\n",
      "0.6242920235492927\n",
      "0.6215664453454283\n",
      "0.6002506984826467\n",
      "0.5914058134757075\n",
      "0.5791260389258788\n",
      "0.5765026706176508\n",
      "0.5717531985585395\n",
      "0.5703350650309089\n",
      "0.5632695526054322\n",
      "0.5574435816960731\n",
      "0.5548699111877413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb7567888424c3dbc529dedd0617263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7498016981175788\n",
      "0.6545367418425997\n",
      "0.6100487365257838\n",
      "0.607665712160602\n",
      "0.5905372825891204\n",
      "0.5752385220197626\n",
      "0.5571505680618279\n",
      "0.5502322645900306\n",
      "0.5402791084952631\n",
      "0.5334305760689935\n",
      "0.5331263761525168\n",
      "0.5215383460462141\n",
      "0.5110089394279633\n",
      "0.5049574599888639\n",
      "0.4937705219566576\n",
      "0.47757983438555973\n",
      "0.4614047174119055\n",
      "0.46007277855447815\n",
      "0.4519998917961033\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b325b56626493198b42c86b24c97e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6810018414105411\n",
      "0.6447280782822613\n",
      "0.6170040729425805\n",
      "0.6005755082534646\n",
      "0.5929388970623705\n",
      "0.584361816641567\n",
      "0.5826193926539804\n",
      "0.5812824400878384\n",
      "0.5634673972709808\n",
      "0.5631013038562275\n",
      "0.5620852667190868\n",
      "0.5583571534747693\n",
      "0.553008708265958\n",
      "0.5495337957296331\n",
      "0.5322209987763775\n",
      "0.516509154920952\n",
      "0.5148366165924173\n",
      "0.49568934573541584\n",
      "0.4870997986875317\n",
      "0.45729731998348566\n",
      "0.45155299109297564\n",
      "0.44952647752051733\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd722779576946b5a9bc5b379cd47cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7836814262892451\n",
      "0.7704716078438715\n",
      "0.6481954083557926\n",
      "0.6344387891716563\n",
      "0.5855917551405102\n",
      "0.5722695899452512\n",
      "0.5568676321208823\n",
      "0.5540270574054211\n",
      "0.5295733412473905\n",
      "0.5218281312132133\n",
      "0.5132848579579204\n",
      "0.5007006958049004\n",
      "0.49815280130506356\n",
      "0.49066865205212534\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700f2d20bc2547e29b431bd730c62736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6616480997705977\n",
      "0.6578122002416822\n",
      "0.6189817936162643\n",
      "0.5940903401806986\n",
      "0.590288069133182\n",
      "0.581898738825091\n",
      "0.5769232249731114\n",
      "0.5650019510798686\n",
      "0.5618972307943377\n",
      "0.5596185188876496\n",
      "0.5507237517033675\n",
      "0.546935433049442\n",
      "0.5460610414174646\n",
      "0.5273632764572124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bce033f349f43b29b47d850b2af9bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.703158718834372\n",
      "0.6204553935516193\n",
      "0.6198256047408206\n",
      "0.6070166085944769\n",
      "0.5995730343104876\n",
      "0.5846234235604006\n",
      "0.573768634060646\n",
      "0.5517843223346758\n",
      "0.5496175779876163\n",
      "0.5437580601229222\n",
      "0.5369885673553979\n",
      "0.5320403355672877\n",
      "0.5251687081096854\n"
     ]
    }
   ],
   "source": [
    "######################## tabelka ##########################################################\n",
    "df = pd.DataFrame({\"Repr 1\": [], \"Repr 10\": [],\n",
    "                   \"Atention Pooling v2 - size = 3\": [], \"Atention Pooling v2 - size = 35\": [], \"Atention Pooling v2 - size = 100\": []})\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "n_times = 1\n",
    "\n",
    "for n_convs in [1, 3, 5]:\n",
    "    for n_channels in [64, 512]:\n",
    "        row = []\n",
    "\n",
    "        #########################\n",
    "        scores = []\n",
    "        for _ in range(n_times):\n",
    "            m =  GraphNeuralNetwork(n_channels, n_convs=n_convs, features_after_layer=26)\n",
    "            m = train_best(m, train_loader1, valid_loader1, epochs=70)\n",
    "            predictions, att = predict(m, test_loader1)\n",
    "            rmse_score = rmse(y_test, predictions.flatten())\n",
    "            scores.append(\"{:.2f}\".format(rmse_score))\n",
    "        row.append(\" | \".join(scores))\n",
    "\n",
    "        #########################\n",
    "        scores = []\n",
    "        for _ in range(n_times):\n",
    "            m =  GraphNeuralNetwork(n_channels, n_convs=n_convs, features_after_layer=25)\n",
    "            m = train_best(m, train_loader10, valid_loader10, epochs= 70)\n",
    "            predictions, att = predict(m, test_loader10)\n",
    "            rmse_score = rmse(y_test, predictions.flatten())\n",
    "            scores.append(\"{:.2f}\".format(rmse_score))\n",
    "        row.append(\" | \".join(scores))\n",
    "\n",
    "        #########################\n",
    "        for vect_size in [3, 35, 100]:\n",
    "            scores = []\n",
    "            for _ in range(n_times):\n",
    "                m =  GraphNeuralNetwork(n_channels, n_convs=n_convs, my_layer=MyAttentionModule4(vect_size), features_after_layer=vect_size)\n",
    "                m = train_best(m, train_loader, valid_loader, epochs=70)\n",
    "                predictions, att = predict(m, test_loader)\n",
    "                rmse_score = rmse(y_test, predictions.flatten())\n",
    "                scores.append(\"{:.2f}\".format(rmse_score))\n",
    "            row.append(\" | \".join(scores))\n",
    "\n",
    "        df.loc[str(n_convs) + \" convs, \" + str(n_channels) + \" channels\"] = row\n",
    "\n",
    "df.to_csv(\"qm9_out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repr 1</th>\n",
       "      <th>Repr 10</th>\n",
       "      <th>Atention Pooling v2 - size = 3</th>\n",
       "      <th>Atention Pooling v2 - size = 35</th>\n",
       "      <th>Atention Pooling v2 - size = 100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1 convs, 64 channels</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 convs, 512 channels</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 convs, 64 channels</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 convs, 512 channels</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 convs, 64 channels</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 convs, 512 channels</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Repr 1 Repr 10 Atention Pooling v2 - size = 3   \n",
       "1 convs, 64 channels    0.47    0.47                           0.38  \\\n",
       "1 convs, 512 channels   0.47    0.47                           0.38   \n",
       "3 convs, 64 channels    0.39    0.39                           0.46   \n",
       "3 convs, 512 channels   0.41    0.39                           0.39   \n",
       "5 convs, 64 channels    0.44    0.44                           0.77   \n",
       "5 convs, 512 channels   0.45    0.45                           0.49   \n",
       "\n",
       "                      Atention Pooling v2 - size = 35   \n",
       "1 convs, 64 channels                             0.38  \\\n",
       "1 convs, 512 channels                            0.41   \n",
       "3 convs, 64 channels                             0.37   \n",
       "3 convs, 512 channels                            0.39   \n",
       "5 convs, 64 channels                             0.53   \n",
       "5 convs, 512 channels                            0.53   \n",
       "\n",
       "                      Atention Pooling v2 - size = 100  \n",
       "1 convs, 64 channels                              0.35  \n",
       "1 convs, 512 channels                             0.37  \n",
       "3 convs, 64 channels                              0.38  \n",
       "3 convs, 512 channels                             0.38  \n",
       "5 convs, 64 channels                              0.56  \n",
       "5 convs, 512 channels                             0.52  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5097fb7fe8ce44b2ae6f777397589c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6029386290882007\n",
      "0.5683466222907125\n",
      "0.5604436025787646\n",
      "0.5376977818992003\n",
      "0.5321830417351598\n",
      "0.4908033227389411\n",
      "0.44490565618354005\n",
      "0.43717932747550264\n",
      "0.41816708144753073\n",
      "0.4130349799112501\n",
      "0.40203166803965873\n",
      "0.3925937594910248\n",
      "0.39\n"
     ]
    }
   ],
   "source": [
    "m =  GraphNeuralNetwork(512, n_convs=3, my_layer=MyAttentionModule4(35), features_after_layer=35)\n",
    "m = train_best(m, train_loader, valid_loader, epochs=70)\n",
    "predictions, att = predict(m, test_loader)\n",
    "rmse_score = rmse(y_test, predictions.flatten())\n",
    "print(\"{:.2f}\".format(rmse_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.39\n"
     ]
    }
   ],
   "source": [
    "###################### atencja #################################\n",
    "\n",
    "df_single = pd.DataFrame({\"AtomicNum\": [], \"Degree\": [], \"TotalNumHs\": [], \"ImplicitValence\": [], \"Hybridization\": [], \"FormalCharge\": [],\n",
    "                          \"IsInRing\": [], \"IsAromatic\": [], \"NumRadicalElectrons\": []})\n",
    "df_single.style.set_caption(\"Hello World\")\n",
    "\n",
    "df_batch = pd.DataFrame({\"AtomicNum\": [], \"Degree\": [], \"TotalNumHs\": [], \"ImplicitValence\": [], \"Hybridization\": [], \"FormalCharge\": [],\n",
    "                          \"IsInRing\": [], \"IsAromatic\": [], \"NumRadicalElectrons\": []})\n",
    "df_batch.style.set_caption(\"Hello World\")\n",
    "\n",
    "preds_batches = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        preds, att = m(x, edge_index, batch)\n",
    "        preds_batches.append(preds.cpu().detach().numpy())\n",
    "        att = att.squeeze()\n",
    "        df_single.loc[len(df_single)] = att[0].tolist()\n",
    "        df_batch.loc[len(df_single)] = torch.mean(gap(att, batch), dim=0).tolist()\n",
    "preds = np.concatenate(preds_batches)\n",
    "\n",
    "rmse_score = rmse(y_test, predictions.flatten())\n",
    "\n",
    "print(f'RMSE = {rmse_score:.2f}')\n",
    "df_single.to_csv(\"qm9_att_single.csv\")\n",
    "df_batch.to_csv(\"qm9_att_batch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtomicNum</th>\n",
       "      <th>Degree</th>\n",
       "      <th>TotalNumHs</th>\n",
       "      <th>ImplicitValence</th>\n",
       "      <th>Hybridization</th>\n",
       "      <th>FormalCharge</th>\n",
       "      <th>IsInRing</th>\n",
       "      <th>IsAromatic</th>\n",
       "      <th>NumRadicalElectrons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtomicNum  Degree  TotalNumHs  ImplicitValence  Hybridization   \n",
       "0       0.47    0.06        0.06             0.08           0.06  \\\n",
       "1       0.55    0.05        0.06             0.07           0.05   \n",
       "2       0.48    0.06        0.06             0.08           0.06   \n",
       "3       0.53    0.05        0.06             0.07           0.05   \n",
       "4       0.49    0.06        0.06             0.08           0.06   \n",
       "5       0.48    0.06        0.06             0.08           0.06   \n",
       "6       0.48    0.06        0.06             0.08           0.06   \n",
       "7       0.49    0.05        0.06             0.07           0.05   \n",
       "8       0.49    0.05        0.06             0.07           0.05   \n",
       "9       0.47    0.06        0.06             0.08           0.06   \n",
       "\n",
       "   FormalCharge  IsInRing  IsAromatic  NumRadicalElectrons  \n",
       "0          0.11      0.06        0.06                 0.06  \n",
       "1          0.09      0.05        0.05                 0.05  \n",
       "2          0.10      0.06        0.06                 0.06  \n",
       "3          0.09      0.05        0.05                 0.05  \n",
       "4          0.10      0.06        0.06                 0.06  \n",
       "5          0.10      0.06        0.06                 0.06  \n",
       "6          0.10      0.06        0.06                 0.06  \n",
       "7          0.10      0.05        0.05                 0.05  \n",
       "8          0.10      0.05        0.05                 0.05  \n",
       "9          0.11      0.06        0.06                 0.06  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_single[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtomicNum</th>\n",
       "      <th>Degree</th>\n",
       "      <th>TotalNumHs</th>\n",
       "      <th>ImplicitValence</th>\n",
       "      <th>Hybridization</th>\n",
       "      <th>FormalCharge</th>\n",
       "      <th>IsInRing</th>\n",
       "      <th>IsAromatic</th>\n",
       "      <th>NumRadicalElectrons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AtomicNum  Degree  TotalNumHs  ImplicitValence  Hybridization   \n",
       "1        0.26    0.08        0.08             0.14           0.07  \\\n",
       "2        0.26    0.08        0.08             0.14           0.07   \n",
       "3        0.26    0.08        0.08             0.14           0.07   \n",
       "4        0.26    0.08        0.08             0.14           0.07   \n",
       "5        0.26    0.08        0.08             0.14           0.07   \n",
       "6        0.26    0.08        0.08             0.15           0.07   \n",
       "7        0.26    0.08        0.08             0.14           0.07   \n",
       "8        0.26    0.08        0.08             0.14           0.07   \n",
       "9        0.26    0.08        0.08             0.14           0.07   \n",
       "10       0.26    0.08        0.08             0.14           0.07   \n",
       "\n",
       "    FormalCharge  IsInRing  IsAromatic  NumRadicalElectrons  \n",
       "1           0.14      0.07        0.07                 0.07  \n",
       "2           0.14      0.07        0.07                 0.07  \n",
       "3           0.14      0.07        0.07                 0.07  \n",
       "4           0.14      0.07        0.07                 0.07  \n",
       "5           0.14      0.07        0.07                 0.07  \n",
       "6           0.14      0.07        0.07                 0.07  \n",
       "7           0.14      0.07        0.07                 0.07  \n",
       "8           0.14      0.07        0.07                 0.07  \n",
       "9           0.14      0.07        0.07                 0.07  \n",
       "10          0.14      0.07        0.07                 0.07  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_batch[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldd23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
